---
title: "Data 605 - Final"
author: "Georgia Galanopoulos"
date: "December 20, 2017"
output: html_document
---

```{r}
library("knitr")
library("gplots")
library("ggplot2")
library("corrplot")
library("MASS")
library("fitdistrplus")
library("Amelia")
library("usdm")
library("dplyr")
library("reshape2")
```

**Pick one of the quantitative independent variables from the training data set (train.csv), and define that variable as X. Pick SalePrice as the dependent variable, and define it as Y for the next analysis.**

```{r}
train = read.csv("train.csv")
kable(head(train))

# Finding all quantitative variables
integers = sapply(train, is.numeric)
integers2 = train[, integers]

# substituting mean for NA values
for(i in 1:ncol(integers2)){
  integers2[is.na(integers2[,i]), i] = mean(integers2[,i], na.rm = TRUE)
}
```

```{r fig.height = 8, fig.width = 8, fig.align = "center"}
correlations = round(cor(integers2),2)
corrplot(correlations, method="circle")
```

Looking at the variables with a low correlation, we can pick one (in this case, Month Sold) and test with a chi-square test to see if it is independent to SalePrice.

$H_0:$ The two variables are independent

$H_A:$ The two variables are related
```{r}
# Independence test
chisq = chisq.test(integers2$SalePrice, integers2$MoSold)
chisq
```

With a p-value greater than 0.05, we fail to reject our null hypothesis, so we will set is as our X variable.
```{r}
Y = train$SalePrice
X = train$MoSold
```


##Probability

**Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the 1st quartile of the X variable, and the small letter "y" is estimated as the 2nd quartile of the Y variable.  Interpret the meaning of all probabilities.**

```{r}
x = 5
y = 163000

SaleMonth = data.frame(X,Y)
nSaleMonth = SaleMonth %>% group_by(X,Y) %>% summarize(n = n())
propSaleMonth = nSaleMonth %>%  ungroup() %>%  mutate(prop = n / sum(n))

marginalMonth =  propSaleMonth %>%  group_by(X) %>% summarize(marginalX = sum(prop))
marginalSale =  propSaleMonth %>% group_by(Y) %>% summarize(marginalY = sum(prop))

prop.marg.SaleMonth = merge(propSaleMonth, marginalMonth, by="X")
prop.marg.SaleMonth = merge(prop.marg.SaleMonth, marginalSale, by="Y")
```

$a.\hspace{5mm} P(X>x | Y>y)$
```{r}
joint.prob = prop.marg.SaleMonth %>%  filter(Y > y, X > x) %>% sum(.$prop)
marg.prob = marginalSale %>% filter(Y > y) %>% sum(.$marginalY)

cond.prob = joint.prob / marg.prob
print(paste("Conditional Probability:", round(cond.prob,4)*100,"%"))
```

$b.\hspace{5mm} P(X>x \& Y>y)$
```{r}
joint.prob = prop.marg.SaleMonth %>%  filter(X > x & Y > y) %>% sum(.$prop)
marg.prob = marginalSale %>% filter(Y > y) %>% sum(.$marginalY)

cond.prob = joint.prob / marg.prob
print(paste("Conditional Probability:", round(cond.prob,4)*100,"%"))
```

$c.\hspace{5mm} P(X<x | Y>y)$
```{r}
joint.prob = prop.marg.SaleMonth %>%  filter(Y > y, X < x) %>% sum(.$prop)
marg.prob = marginalSale %>% filter(Y > y) %>% sum(.$marginalY)

cond.prob = joint.prob / marg.prob
print(paste("Conditional Probability:", round(cond.prob,4)*100,"%"))
```

**Does splitting the training data in this fashion make them independent? In other words, does $P(XY)=P(X)P(Y)$? Check mathematically, and then evaluate by running a Chi Square test for association.**
```{r}
PX = as.data.frame(table(prop.marg.SaleMonth$X)/length(prop.marg.SaleMonth$X))
names(PX) = c("X", "ProbX")
PY = as.data.frame(table(prop.marg.SaleMonth$Y)/length(prop.marg.SaleMonth$Y))
names(PY) = c("Y", "ProbY")

mergedProbs = merge(propSaleMonth, PX, by="X")
mergedProbs = merge(mergedProbs, PY, by="Y")
mergedProbs = mergedProbs %>% mutate(PXPY = ProbX*ProbY)
kable(head(mergedProbs))

chisq = chisq.test(mergedProbs$prop, mergedProbs$PXPY)
chisq
```

$H_0:$ The two probability variables are independent

$H_A:$ The two probability variables are related

With a p-value less thant 0.05, we reject the null that the two probability variables are independent to conclude that $P(XY)=P(X)P(Y)$.

##Descriptive and Inferential Statistics

**Provide univariate descriptive statistics and appropriate plots for both variables.**
```{r}
# Sale Price
summary(Y)
ggplot(train, aes(x= SalePrice))+ geom_density()


# Months Sold
summary(X)
ggplot(train, aes(x= MoSold))+ geom_density()
```


**Provide a scatterplot of X and Y.**
```{r}
plot(Y~X, main="Sale Price vs Month", xlab = "Month", ylab = "Sale Price")
ggplot(train, aes(x = MoSold, y = SalePrice, group = MoSold))+geom_boxplot(aes(fill=MoSold))
```

**Transform both variables simultaneously using Box-Cox transformations.**
```{r}
box = boxcox(Y ~ X)
cox = data.frame(box$x, box$y)
Cox = cox[with(cox, order(-cox$box.y)),]

lambdaX = Cox[1, "box.x"]
lambdaY = Cox[1, "box.y"]

boxedX = (X^ lambdaY - 1)/lambdaY
boxedY = (Y^ lambdaX - 1)/lambdaX

par(mfrow = c(1,2))
boxplot(boxedY ~ X, main = "Box-Cox Transformation Plot", ylab="Transformed Y", xlab="Non-Transformed X")
boxplot(boxedY ~ boxedX, main = "Box-Cox Transformation Plot", ylab="Transformed Y", xlab="Transformed X")
```

##Linear Algebra and Correlation

**Using at least three untransformed variables, build a correlation matrix.  Invert your correlation matrix. (This is known as the precision matrix and contains variance inflation factors on the diagonal).**
```{r}
# Selecting three untransformed variables and build correlation matrix
precormat = train[c("OverallQual", "GarageArea", "BsmtUnfSF")]
cormat = cor(precormat)
cormat

# Invert correlation matrix
precmat = solve(cormat)
precmat
```
The vif values on the diagonal are low, 1.1-1.5, so there does not appear to be a multicollinearity issue with these three variables.

**Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix.**
```{r}
# Cormat x Precmat
cormat%*%precmat

# Precmat x Cormat
precmat%*%cormat
```


##Calculus-Based Probability & Statistics

**For your non-transformed independent variable, location shift (if necessary) it so that the minimum value is above zero. Then load the MASS package and run fitdistr to fit a density function of your choice.**

Since this portion of the assignment does not require the variable to be independent of the SalePrice, I will choose one other than MoSold, since I think it will fit the assignment better. As such, Garage Area was selected, instead.

```{r}
X = train$GarageArea

# Location shift if necessary
shift = function(X) {
  if (min(X)==0){
    X = X + 1
    } else if (min(X)<0){
      mindiff = 0-(min(X))
      X = X + (mindiff+1)
    }
  return(X)
}

X = shift(X)
plot(X)
```

Picking a distribution. Cauchy seems to fit the best.
```{r}
plotdist(X, histo = TRUE, demp = TRUE)
wei_dist  = fitdist(X, "weibull")
cau_dist = fitdist(X, "cauchy")
gam_dist  = fitdist(X, "gamma")
Log_dist = fitdist(X, "lnorm")


plot.legend = c("Weibull", "Cauchy", "Gamma", "Log Normal")
denscomp(list(wei_dist, cau_dist, gam_dist, Log_dist), legendtext = plot.legend)
cdfcomp(list(wei_dist, cau_dist, gam_dist, Log_dist), legendtext = plot.legend)
qqcomp(list(wei_dist, cau_dist, gam_dist, Log_dist), legendtext = plot.legend)
ppcomp(list(wei_dist, cau_dist, gam_dist, Log_dist), legendtext = plot.legend)
```

**Find the optimal value of the parameters for this distribution, and then take 1000 samples from this distribution (e.g., rexp(1000, lambda) for an exponential). Plot a histogram and compare it with a histogram of your non-transformed original variable.**

Cauchy parameters: $x_0$ and $\gamma>0$ where the optimal values are $location = 476.0183$ and the $scale=110.2430$. Values are rounded for histogram.
```{r}
# Optimal values
summary(cau_dist)

# Take Samples
cau_samp = rcauchy(1000, 476, 110)
```

```{r}
# Plot comparisons
par(mfrow =c(1,2))
cuts = quantile(cau_samp, c(.1,.9))
hist(cau_samp[cau_samp>=cuts[1] & cau_samp<=cuts[2]], main = "Hist of Cauchy Samples", xlab = "Samples", ylab = "Count")

hist(train$GarageArea, main = "Hist of Non-Transformed Variable", xlab = "Garage Area", ylab = "Count")
```

##Modeling

**Build some type of regression model and submit your model to the competition board.  Provide your complete model summary and results with analysis.**

Regression model to determine the Sale Price. First see what variables to remove. Start with the variables that have too many NA values.
```{r}
# View variables with NA values
missmap(train, main = "Missing values vs observed")

# Remove NA variables
removed = names(train) %in% c("PoolQC","MiscFeature", "Alley", "Fence", "FireplaceQu", "LotFrontage", "Id")
newtrain = na.omit(train[!removed])

reg = lm(SalePrice~., data = newtrain)
stepreg = step(reg)
summary(stepreg)
```

Then remove variables with high variance inflation factors.
```{r}
vifs = car::vif(stepreg)
vifs = tibble::rownames_to_column(as.data.frame(vifs), "names")

# Remove vars with VIF > 4 and re-build model
while(any(vifs$GVIF > 4)){
  # Removing top VIF > 4
  ordered = vifs[order(vifs$GVIF, decreasing =T),][-1,]
  vifnames = ordered$names
  # Rebuilding model
  newreg = as.formula(paste("SalePrice ~ ", paste(vifnames, collapse=" + "), sep=""))
  testreg = lm(newreg, data=newtrain)
  # Recalculating VIFs
  vifs = car::vif(testreg)
  vifs = tibble::rownames_to_column(as.data.frame(vifs), "names")
}

lm.res = summary(testreg)
```

Remove non-significant variables, again.
```{r}
#step(testreg)
stepreg = lm(SalePrice ~ GarageArea + X1stFlrSF + OverallQual + KitchenQual + 
               MasVnrType + X2ndFlrSF + Condition1 + LandSlope + YearRemodAdd + 
               Condition2 + LandContour + MasVnrArea + MSZoning + SaleType + 
               LotArea + BedroomAbvGr + BsmtExposure + MSSubClass + Functional + 
               Fireplaces + BsmtUnfSF + KitchenAbvGr + LotConfig + Street + 
               OverallCond + WoodDeckSF + PoolArea + ScreenPorch + Utilities, data = newtrain)

lm.step.reg = summary(stepreg)
lm.step.reg

#plots
resids = lm.step.reg$residuals
hist(resids, main = "Regression Residuals")
qqnorm(resids)
qqline(resids, col = "red")
```

Looking at the residual plots, it becomes more clear that this regression model does better with houses whose Sale Price falls within certain parameters, more specifically, average pricing. The residuals are too off for houses with too high or too low a price.

```{r}
# small p-value
data.frame(lm.step.reg$coef[coef(lm.step.reg)[,4] <= .05,])
```

Overall results:

The model explains approximately 83% of the variability, which is pretty decent for a regression model when not taking into consideration the variables and their significance. The majority of the variables that did not appear as significant in the model were not removed because they were factors. In order to remove a factor, the entire variable would have to be removed, which severely decreased the R-squared value of the model. As such, using only the variables that were deemed significant, we can come to certain conclusions about the Sale Price: 

Increase:

Overall Quality and Overall Condition of the house seemed significant. With all variables held constant, one unit increase in both Quality and Condition leads to an increase of 1.6 and 1.8 units (for each respective variable) in Sale Price. A unit increase in Garage Area increased the Sale Price by 3.7 units. An unit increase in area on the first floor increased the Sale Price by 7 units. The unit increase in Condition1 (nearness to railroad/main road) increased Sale Price by 1.8 units across more than one level. Unit increase in Land Contouring increased Sale Price by at 1-2 units, the highest being the the HLS level with a 2.7 increase. Unit increase in Zoning increased Sale Price, as well, by 2-3 units across 2 levels. Unit increase in Wood Deck area and Screen Porch are increased Sale Price by 2.4 and 3.7 units respectively. Also, the unit increase in remodel date (YearRemodAdd) increased the Sale Price by 1.6 units

Decrease:

Surprisingly, overall, a unit increase in Kitchen Quality across various factors lead to a 4-5 unit decrease in Sale Price. Unit increase in Pool Area also decreased the Sale Price by 4 units. A unit increase in LandSlope across the Sev level decreased the Sale Price by 2.8 units. A unit increase in Condition2 (nearness to a second railroad/main road) let to a 2.5 and 7.4 unit decrease across the PosN and Feedr levels respectively. In contrast with the increase in Condition1, Sale Price appears to drop when there is significant access to transportation/transportation routes. A unit increase in unfinished Basement area shows a 1.1 unit decrease in Sale Price.

Mixed:

Because some variables that scored as significant had factors, they may have had an different impact on Sale Price across levels. An example of that is Basement Exposure, where a garden level basement led to a 1.3 unit increase in Sale Price, but a walkout basement led to a 8.8 unit decrease, instead.


Looking at the variables overall, it appears that some conclusions could be drawn as to the patterns determining sale price. The larger the lot area or the garage area, the condition/quality of the house, the higher the sale price. This means the more unfinished area (basement, bedrooms, etc), the lower the price. Zoning and distance from main roads/railroads have an impact as well, so location (and what it has to offer) is important. But too many railroads/main roads, and the sale price drops. The more recent any remodeling has been done, the higher the price and if the property has more wood deck and screen porch area, the price increases as well. The only interesting thing to note is that regardless of the levels across kitchen quality, the Sale Price decreases.






